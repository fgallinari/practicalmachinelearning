---
title: "Practical Machine Learning Course Project"
author: "Felipe Gallinari"
date: "March 9, 2016"
output: html_document
---

###Introduction
This document describes the steps used to predict the classes of the data from the final assignment of the Coursera Practical Machine Learning Course.

>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. (...) One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify **how well** they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The above text was copied from the course page.

###Before Starting

Before we start building our model, we need to download the data from the website and also loading the libraries we are going to use.
The training data can be downloaded here:

- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

And the testing data from here:

- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The seed is set here for *reproducibility*.
```{r, message=F}
library(caret)
library(doParallel)

set.seed(123456)

training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

###Cleansing Data

After we load and explore the data, it's easy to see that there are some columns that have missing values. In other words, if there is any NA or blank observation in the column, we will not use it as a good predictor.
Also, we want only predictors that are related to device measures (arm, belt, dumbbell, forearm) and the objective (*classe*) itself

```{r}
isBadColumn <- sapply(training, function(obs) any(is.na(obs) | obs == ""))
isDeviceMeasureOrObjective <- grepl("belt|forearm|dumbbell|arm|classe", names(training))
isGoodPredictor <- !isBadColumn & isDeviceMeasureOrObjective
```

Now, we start using a new Data Frame, with 53 features.

```{r}
training.set <- training[,isGoodPredictor]
```

Make the objective variable a factor:

```{r}
training.set$classe <- as.factor(training.set$classe)
```

To get a better result, we will split our training set, to get a validation set before testing it. Here we are going to use 70% for the training set and the remaining 30% to validate.

```{r}
inTrain = createDataPartition(training.set$classe, p = 0.7, list=FALSE)

validation.set <- training.set[-inTrain,]
training.set <- training.set[inTrain,]
```

Our train control is set to cross validation with 5 folds, parallel computation allowed, saving all the hold-out predictions for each resample and computing class probabilities for each resample.

```{r}
trControl <- trainControl(classProbs=TRUE, savePredictions=TRUE, allowParallel=TRUE, method="cv", number=5)
```

###Building the Models
To get the models built faster, we are using the parallel library to use all the cores but one.
For this project, we are trying 3 different methods:
- Random Forests
- Gradient Boost
- Support Vector Machines with Radial Kernel
- And a combine of all the above
```{r}
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

system.time(rf.model <- train(classe ~., method = "rf", data = training.set, trControl = trControl))
system.time(gbm.model <- train(classe ~., method = "gbm", data = training.set, trControl = trControl))
system.time(svm.model <- train(classe ~., method = "svmRadial", data = training.set, trControl = trControl))
```

After calculating them, we will predict with our validation set. Also, we are ensembling the 3 models into a random forest combine model, using the same train control.

```{r}
rf.result <- predict(rf.model, newdata = validation.set)
gbm.result <- predict(gbm.model, newdata = validation.set)
svm.result <- predict(svm.model, newdata = validation.set)
comb.df <- data.frame(rf.result, gbm.result, svm.result, classe = validation.set$classe)
system.time(comb.model <- train(classe ~., method="rf", data = comb.df, trControl = trControl))

comb.result <- predict(comb.model, newdata = validation.set)
stopCluster(cluster)
```


Now that we have all the models, we can compare the Accuracy of each of them.

```{r}
confusionMatrix(validation.set$classe, rf.result)$overall['Accuracy']
varImp(rf.model)
rf.model$finalModel
confusionMatrix(validation.set$classe, gbm.result)$overall['Accuracy']
confusionMatrix(validation.set$classe, svm.result)$overall['Accuracy']
confusionMatrix(validation.set$classe, comb.result)$overall['Accuracy']
```

###Test Prediction

As we can see above, both the rf.model and the comb.model had a high accuracy, in this case the same. For simplicity matters, we will keep the rf.model to predict the classes for the final test.
Also, the estimated error rate should be something below 1%, as we should see next.

```{r}
rf.model$finalModel
testing.result <- predict(rf.model, newdata = testing)
testing.result
```

To be honest, I didn't expect to get all the answers correct when testing the predictions above.

###Conclusion

Although all the models had great chances to be chosen, the reason the random forest was picked is that this method minimizes the variance, therefore the overfitting. In other words, if we have an high accuracy in out training stage, it will continue doing a good job at testing with new data.

###Environment Used

```{r}
sessionInfo()
```
